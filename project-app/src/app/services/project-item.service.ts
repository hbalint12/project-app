import { Injectable } from '@angular/core';

@Injectable({
    providedIn: 'root'
})
export class ProjectItemService{
    projectItems = [
        {
            id: 1,
            title: 'Time Series Analysis with Deep Learning',
            subheader: 'Stock price prediction with Apache Spark and LSTM',
            summary: 'The subject of my project is time series analysis with a deep learning approach and how the examined group of algorithms could be transferred to a distributed system in order to predict stock and foreign exchange prices.',
            abstract: "",
            description: [ 
                "<h5>Abstract</h5><br>",
                '<article style="text-indent:50px;">During my research, I acquired a deep understanding of state-of-the-art deep learning algorithms, I designed and implemented a LSTM (Long Short-Term Memory) algorithm using the TensorFlow library. This model is able to learn and predict real stock market prices. Deep learning based prediction approaches need a significant amount of data to work on, which ends up increasing also the computation time greatly. To solve these issues, I used adistributed solution of Apache Spark to process the data, because it provides a well documented interface, which enables coding the cluster and also it supports parallel dataprocessing as well. The framework requires a distributed file system, that is supportedby Spark. For that I chose the Hadoop Distributed File System. I trained and evaluated my model on the distributed system, my results are accompanied with figures and tables in order to provide a better understanding of my task. The following description is short summary of the project work.</article>',
                "<br>",
                "<h5>1. Introduction</h5><br>",
                "<article>Prediction of stock prices can be crucial for investors, who would able to leverage with the help of this information. The trends are affected by several factors (physical or physiological, rational or irrational behaviour), which makes the accurate prediction very hard. For the discovery of these patterns, hidden structures and connections the most effective way is the machine and deep learning approach. The stock and currency price prediction based on previous trends with the assumption of existing patterns in the data is called technical analysis. There are many time series analyis methods but with the rapid development of technology the deep learning algorithms reached the best results, especially the Long Short-Term Memory model, which will be discussed hereby.</article>",
                "<br>",
                "<h5>2. Recurrent Neural Networks (RNN)</h5><br>",
                "<article>The time series data, like the stock prices, are extremely complex and constantly changing. Because the deep learning models don't make assumptions on patterns in the data and they are robust on noise, which means on the random deviations in the series, they are proper choice for time series analysis. The Recurrent Neural Network is well-known model for these kind of problems. In contrast to tradicional neural networks this architecture facilitate the processing of sequences. <br><br><div class='img-with-text'><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/rnn.png' alt='RNN'/><p>1. Recurrent Neural Network</p></div><br>The architecture of the neural network was designed to be able to condsider events and to use that information in the future. This model became popular on several fields, like speech recognition or translation. However, it has limits, which can be shown by a language modelling issue. In case the model should predict the last word of the sentence: \"The clouds are in the sky\", it doesn't require any further context. The prediction of the following sentence does: \"I speak fluent French\", because the information, which language I speak, could have been told by serveral sentences before and that is the problem. The gap between the necessary information and the usage of it is way too large and the model cannot learn that. In details it means the following. During the backpropagation the algorithm uses the value of the gradient to refresh the weights of the neural network. After a while this gradient value decreases to such low that it will not have any affect on the learning. This happens mostly in the early moduls of the RNN, which stop learning and due to that the model cannot learn the long term dependencies. This limit is exceeded by the modified version of this model, which is called the Long Short-Term Memory.</article>",
                "<br>",
                "<h5>3. Long Short-Term Memory (LSTM)</h5><br>",
                "<article>The LSTM model was designed in 1997 in order to overcome the limitation of the RNNs and be able to learn long term dependencies. The structute is similarly to the RNN's is chained but the inside of the moduls are much more complex. <br><br><div class='img-with-text'><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/lstm.png' alt='LSTM'/><p>2. Long Short-Term Memory</p></div><br><div class='img-with-text'><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/lstm_parts.png' alt='LSTM parts'/><p>3. Long Short-Term Memory notation</p></div><br>The greatest leverage of this model compared to the RNN that it is solving the vanishing gradient problem. The key part of the LSTM model is the cell state, which is the arrow above on the picture 4, notated with C at the start and the end. Due to the inner structure of the modul the model can add and remove information from the cell state with the so called gates. The first gate highlighted on the 4. picture, is the forget gate. It can deiced whith the sigmoid function, which information will be neglected by the model.<br><br><div class='img-with-text'><img class='col-12 col-md-5 offset-md-2 offset-0' src='../assets/forget_gate.png' alt='Forget gate'/><p>4. LSTM forget gate</p></div><br></article>",
                "<article>The next step highlighted on the 5. picture is the input gate. The model decides what information should it store into the cell state whithin two steps. In the first step a sigmoid neural layer decides, which values should be updated, that goes to the i output. The new values will be determined by the tanh layer to the C output.</div><br><div class='img-with-text'><img class='col-12 col-md-5 offset-md-2 offset-0' src='../assets/input_gate.png' alt='Input gate'/><p>5. Input gate</p></div><br></article>",
                "<article>The last gate of the modul determines what goes on the output of the modul unit. The cell state will be filtered with a tanh layer and the concateneted input, from the previous modul's output and the new x input of the current unit, will be filtered with a sigmoid layer.<br><div class='img-with-text'><img class='col-12 col-md-5 offset-md-2 offset-0' src='../assets/last_gate.png' alt='Last gate of the modul'/><p>6. Last gate of the modul</p></div><br></article>",
                "<br>",
                "<h5>4. Python implementation</h5><br>",
                "<article>The model was implemented in Python, because the available libraries make the process much easier and for the future deployment into the distributed environment the API supports this language as well.</article>",
                "<article>During the implementation of the algorithm a data source had to be choosen. The Yahoo Finance provides a proper API, with that the stock and currency prices are available. With the help of the Pandas Python library, the data can be downloaded with setting some parameters and stored into a DataFrame immediately with the following code.<br><br><div class='img-with-text'><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/data_load_code.png' alt='Code for data loading'/><br><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/apple_data.png' alt='Code for data loading'/><p>7. Traits and values of the Apple stocks</p></div></article>",
                "<article>A plot can be useful to understand the data properly. This was done by the following code lines.<br><br><div class='img-with-text'><img class='col-12 col-md-5 offset-md-2 offset-0' src='../assets/plot_code.png' alt='Code for data plotting'/><br><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/apple_plot.png' alt='Plot'/><p>8. Apple historical data</p></div></article>",
                "<article>The model will get only a single feature, the closing price. In order to use that a new DataFrame had to be created with filtering the old data set and scle the values between 0 and 1 for better model performance.<br><br><div class='img-with-text'><img class='col-12 col-md-5 offset-md-2 offset-0' src='../assets/scale_code.png' alt='Code for data scaling'/></div><br>For the learning data set I picked the 80% of the available historical data. The created data set was split into the x_train array, which contains the independent variables and the y_train, that contains the target variables. The x_train array was filled with array, that contains the closing prices of stocks for a 60 day interval, while the y_train stored the closing price of the 61. day. This way the algorithm will estimate the 61. value based on 60 days of closing prices.<br><br><div class='img-with-text'><img class='col-12 col-md-5 offset-md-2 offset-0' src='../assets/traning_window.png' alt='Code for data split'/></div><br> In order to be able to use the x_train and y_train arrays for teaching the LSTM model they need to be transformed into Numpy arrays. The input of the LSTM model is strict. It needs to be 3 dimension, compound of the samples, timestamps and the number of features. As the x_train is 2 dimensional, it needs to be reshaped. The number of samples is equal to the number of rows, the number of timestamps is 60, and the number of features is 1, the closing price.<br><br><div class='img-with-text'><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/reshape_code.png' alt='Code for reshaping'/></div><br>The test data set was created similarly to previous process, but in this case the rest 20% of the scaled data was used.<br><br><div class='img-with-text'><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/test_code.png' alt='Code for test dataset'/></div><br></article>",
                "<article>The LSTM was created with 50 neurons, modul units that was discussed previously. For the mode compilation an optimizer and a loss function has to be given, which measures the performance of the model during the learning process. For starting the learning process additional parameter is required, the batch_size and the epochs. The batch_size is a hyperparameter of the gradient descent, which gives the number of samples on that the model has to run through before it updates the parameters inside. The epochs is an other hyperparameter for the gradient descent, that gives the total number of times the model runs through the whole learning data set.<br><br><div class='img-with-text'><img class='col-12 col-md-6 offset-md-2 offset-0' src='../assets/lstm_code.png' alt='Code for lstm'/></div><br><br><br><div class='img-with-text'><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/model_summary.png' alt='Model summary'/><p>9. The output of the model.summary method</p><img class='col-12 col-md-6 offset-md-2 offset-0' src='../assets/visual_model.png' alt='Model visual'/><p>10. The visualization of the model</p></div>After running the prediction on the test data set, the result was scaled back to be more interpretable and measured by the root mean squared error.<br><br><div class='img-with-text'><img class='col-12 col-md-5 offset-md-2 offset-0' src='../assets/rmse_code.png' alt='Code for RMSE'/></div><br>The predicted values were plotted along the real ones and the training data set to get more insight into the results.<br><br><div class='img-with-text'><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/test_plot.png' alt='Result plot'/><p>11. Performance of the model</p></div><br>For the challenge of increasing amount of data, faster learning process and better scalability, an implementation deployed on a distributed architecture could be a solution. In the next sections introduced distributed solutions aims the examination of the performance and scalability of the original model in such an environment.</article>",
                "<h5>5. Distributed environment</h5><br>",
                "<article>The virtual environment was created with VirtualBox, a free and opensource hypervisor software, that makes possible to run virtual machines (VM) on the host machine. The hypervisor based virtualization provides strong error isolation and security. The applications on the VMs cannot have any bad affect on apps of an other VM, even when the operating system crashes, but it has higher memory and CPU costs. In order to keep my distributed environment safe and clear from the host machine I picked this technology. For the distributed system I picked the Apache Spark framework, that won't be discussed here any further just like the Hadoop framework and the terms as cluster, distributed. I make the assumption for some basic knowledge of them. For the system I created 3 VMs in the VirtualBox and installed the latest Linux distribution. Only the partition for the machines should be increased for at least 15-20 GB to be able to install everything smoothly. Regarding the roles there is one master and two slave machines in the system. In order to let machines comunicate the Host-only network adapter should be set. The IPv4 address and the network mask will be set automatically, and with enabling the DHCP server the addresses will be provided automatically. The screeshots show a VirtualBox with Hungarian language setup, but the structure of the software is same in every language.<br><br><br><div class='img-with-text'><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/vm_network_setup.png' alt='VM setup'/><p>12. The setup of network properties of VMs</p></div><br>For all of the created VMs I configured a Host-only adapter, with that I added the machine to the network, just like on the picture below.<br><br><br><div class='img-with-text'><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/host_only_setup.png' alt='Host-only setup'/><p>12. The setup of Host-only adapters</p></div>I created the master machine and the spark-master.vdi file had to be copied and the UUID had to be changed and the machine renamed to spark-slave1.vdi. The second slave was created similarly. In the /etc/hostname file I set the name for the machines (ubuntu-master, ubuntu-slave1 and ubuntu-slave2). The host information were set in the /etc/hosts file for every VM. The Spark and Hadoop versions has to be downloaded. Because this system is only for demonstration I used the ssh-keygen Linux command, that creates the id_rsa.pub file. From this file the key has to be added to the authorized_keys file, just the machines be able to communicate on the network. The Spark configuration is helped by several template files. On the master VM in the slaves.template file the names of the slave machines were added and the environmental variables were set in the spark-env.sh file, showed by the picture below.<br><br><br><div class='img-with-text'><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/env_setup.png' alt='Env setup'/><p>13. The setup of env variables</p></div>The Spark cluster can be started by running the /sbin/start-all.sh on the default 8080 port. The web UI i available only on the master VM<br><br><br><div class='img-with-text'><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/web_ui.png' alt='ui'/><p>14. The web UI on the 8080 port</p></div>In order to be able to monitor the already run apps on the cluster the history server should be started by running the start-history-server.sh, but first the /tmp/spark-events folder must be created. The server's default port is 18080.</article>",
                "<article>After that the Hadoop distributed file system was configured. First of all the NameNode was parametrized in the /hadoop/etc/hadoop/core-site.xml, where the name and the hostname of the master node with the port number needs to be given.<br><br><br><div class='img-with-text'><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/namenode_setup.png' alt='NameNode setup'/><p>15. The NameNode configuration</p></div><br>In the hdfs-site.xml file the routes for the nodes has to be setup and the dfs.replication parameter, which indicates the number of replication of the data stored in the filesystem.<br><br><br><div class='img-with-text'><img class='col-12 col-md-7 offset-md-2 offset-0' src='../assets/hdfs_site_setup.png' alt='hdfs-site setup'/><p>16. The parameters of HDFS</p></div><br>All of the above mentioned files were copied on the other nodes as well, due to previous SSH key setup without any password quick and smooth. The HDFS filesystem has to be formated from the master node, where I ran the 'hdfs namenode -format' command. The services of the filesystem can be started by running the start-dfs.sh script. The currently runnning services can be checked with the jps commend, just like on the pictures below.<br><br><br><div class='img-with-text'><img class='col-12 col-md-3 offset-md-2 offset-0' src='../assets/hdfs_services_master.png' alt='hdfs-services'/><p>17. The services of HDFS on the master node</p><img class='col-12 col-md-2 offset-md-2 offset-0' src='../assets/hdfs_services_slave.png' alt='hdfs-services'/><p>18. The services of HDFS on the slave nodes</p></div><br>A web UI is available for the filesystem as well on the 9870 default port on the master node. Th management can be done by POSIX similar commands, like 'hdfs dfs -put from_path_of_file /', which copies the file to the HDFS.<br><br><br><div class='img-with-text'><img class='col-12 col-md-11 offset-md-2 offset-0' src='../assets/hadoop_ui.png' alt='Hadoop ui'/><p>19. The Hadoop web UI</p></div></article>",
                "<article></article>"
            ],
            category: 'Distributed Deep  Learning',
            categoryList: ['Apache Spark','Distributed Deep Learning'],
            technologyList: ['Apache Spark','Python','LSTM']
        }
    ]

    get(){
        return this.projectItems;
    }

    getById(id){
        return this.projectItems.filter(i => i.id == id).pop();
    }

    add(projectItem){
        this.projectItems.push(projectItem);
    }

    delete(projectItem){
        const index = this.projectItems.indexOf(projectItem);
        if(index >=0){
            this.projectItems.splice(index,1);
        }
    }
}